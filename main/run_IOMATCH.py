import os
import sys
import time

import numpy as np
import rich
import torch

sys.path.append("./")
from configs import IOMATCHConfig
from datasets.cifar import CIFAR, CIFAR_STRONG, load_CIFAR
from datasets.svhn import SVHN, SVHN_STRONG, load_SVHN
from datasets.tiny import TinyImageNet, TinyImageNet_STRONG, load_tiny
from datasets.transforms import SemiAugment, TestAugment
from models import WRN, densenet121, inceptionv4, vgg16_bn
from tasks.classification_IOMATCH import Classification
from utils.gpu import set_gpu
from utils.initialization import initialize_weights
from utils.logging import get_rich_logger
from utils.wandb import configure_wandb
# 기존 import 아래에 추가
from datasets.ptbxl import get_ptbxl, PTBXLDataset
from models.resnet1d import ResNet1D

NUM_CLASSES = {
    "cifar10": 6,
    "cifar100": 50,
    "svhn": 6,
    "tiny": 100,
    "ptbxl": 3,  # [추가]
}

AUGMENTS = {"semi": SemiAugment, "test": TestAugment}


def main():
    """Main function for single/distributed linear classification."""

    config = IOMATCHConfig.parse_arguments()
    set_gpu(config)

    rich.print(config.__dict__)
    config.save()

    np.random.seed(config.seed)
    torch.manual_seed(config.seed)
    torch.backends.cudnn.deterministic = False
    torch.backends.cudnn.benchmark = True

    rich.print("Single GPU training.")
    main_worker(0, config=config)  # single machine, single gpu


def main_worker(local_rank: int, config: object):
    """Single process."""

    torch.cuda.set_device(local_rank)
    num_classes = NUM_CLASSES[config.data]

    # Networks
    if config.backbone_type in ["wide28_10", "wide28_2"]:
        model = WRN(
            width=int(config.backbone_type.split("_")[-1]), num_classes=num_classes
        )
    elif config.backbone_type == "densenet121":
        model = densenet121(num_class=num_classes)
    elif config.backbone_type == "vgg16_bn":
        model = vgg16_bn(num_class=num_classes)
    elif config.backbone_type == "inceptionv4":
        model = inceptionv4(class_nums=num_classes)
    elif config.backbone_type == "resnet1d":
        model = ResNet1D(num_classes=num_classes, normalize=False) # FixMatch는 보통 normalize 안씀 (확인 필요)
        # [수정] IOMatch 호환성을 위한 패치
        # ResNet1D의 마지막 레이어(fc 또는 linear)를 'output'이라는 이름으로 연결해줍니다.
        if hasattr(model, 'fc'):
            model.output = model.fc
        elif hasattr(model, 'linear'):
            model.output = model.linear
        else:
            # 만약 둘 다 아니라면 직접 확인이 필요하므로 에러 메시지 출력
            raise AttributeError("ResNet1D 모델에서 마지막 레이어(fc 또는 linear)를 찾을 수 없습니다.")
    
    else:
        raise NotImplementedError

    # create logger
    if local_rank == 0:
        logfile = os.path.join(config.checkpoint_dir, "main.log")
        logger = get_rich_logger(logfile=logfile)
        if config.enable_wandb:
            configure_wandb(
                name=f"{config.task} : {config.hash}",
                project=f"SafeSSL-Calibration-{config.data}-{config.task}",
                config=config,
            )
    else:
        logger = None

    # Sub-Network Plus
    import torch.nn as nn

    setattr(
        model,
        "mlp_proj",
        nn.Sequential(
            nn.Linear(model.output.in_features, model.output.in_features),
            nn.ReLU(),
            nn.Linear(model.output.in_features, model.output.in_features),
        ),
    )
    setattr(
        model,
        "mb_classifiers",
        nn.Linear(model.output.in_features, int(model.class_num * 2)),
    )
    setattr(
        model,
        "openset_classifier",
        nn.Linear(model.output.in_features, int(model.class_num + 1)),
    )

    initialize_weights(model)

    # Data (transforms & datasets)
    trans_kwargs = dict(
        size=config.input_size, data=config.data, impl=config.augmentation
    )
    train_trans = AUGMENTS[config.train_augment](**trans_kwargs)
    test_trans = AUGMENTS[config.test_augment](**trans_kwargs)

    if config.data in ["cifar10", "cifar100"]:

        datasets, _ = load_CIFAR(
            root=config.root,
            data_name=config.data,
            n_valid_per_class=config.n_valid_per_class,
            seed=config.seed,
            n_label_per_class=config.n_label_per_class,
            mismatch_ratio=config.mismatch_ratio,
            logger=logger,
        )

        labeled_set = CIFAR(
            data_name=config.data, dataset=datasets["l_train"], transform=train_trans
        )
        unlabeled_set = CIFAR_STRONG(
            data_name=config.data, dataset=datasets["u_train"], transform=train_trans
        )
        eval_set = CIFAR(
            data_name=config.data, dataset=datasets["validation"], transform=test_trans
        )
        test_set = CIFAR(
            data_name=config.data, dataset=datasets["test"], transform=test_trans
        )
        open_test_set = CIFAR(
            data_name=config.data, dataset=datasets["test_total"], transform=test_trans
        )

    elif config.data == "tiny":

        datasets, _ = load_tiny(
            root=config.root,
            n_label_per_class=config.n_label_per_class,
            n_valid_per_class=config.n_valid_per_class,
            mismatch_ratio=config.mismatch_ratio,
            random_state=config.seed,
            logger=logger,
        )

        labeled_set = TinyImageNet(
            data_name=config.data, dataset=datasets["l_train"], transform=train_trans
        )
        unlabeled_set = TinyImageNet_STRONG(
            data_name=config.data, dataset=datasets["u_train"], transform=train_trans
        )
        eval_set = TinyImageNet(
            data_name=config.data, dataset=datasets["validation"], transform=test_trans
        )
        test_set = TinyImageNet(
            data_name=config.data, dataset=datasets["test"], transform=test_trans
        )
        open_test_set = TinyImageNet(
            data_name=config.data, dataset=datasets["test_total"], transform=test_trans
        )
    elif config.data == "ptbxl":
        # 1. PTB-XL 원본 데이터 로드 (딕셔너리 형태)
        datasets = get_ptbxl(
            root=config.root,
            n_label_per_class=config.n_label_per_class,
            mismatch_ratio=config.mismatch_ratio,  # Open Set 비율 (Unknown Class 비율)
            logger=logger
        )

        # 2. Labeled Dataset (학습용, 소량)
        labeled_set = PTBXLDataset(
            data=datasets["l_train"]["x"],
            labels=datasets["l_train"]["y"],
            mode='labeled'
        )

        # 3. Unlabeled Dataset (학습용, 대량, IOMatch의 핵심)
        # 아까 수정한 __getitem__이 'weak_img', 'strong_img'를 반환함
        unlabeled_set = PTBXLDataset(
            data=datasets["u_train"]["x"],
            labels=datasets["u_train"]["y"],
            mode='unlabeled' 
        )

        # 4. Validation Set (검증용)
        eval_set = PTBXLDataset(
            data=datasets["validation"]["x"],
            labels=datasets["validation"]["y"],
            mode='test'
        )

        # 5. Test Set (Known Classes Only - 닫힌 세상 평가)
        test_set = PTBXLDataset(
            data=datasets["test"]["x"],
            labels=datasets["test"]["y"],
            mode='test'
        )

        # 6. Open Set Test (Known + Unknown Classes - 열린 세상 평가)
        # IOMatch는 Unknown을 얼마나 잘 구별하는지도 평가해야 함
        open_test_set = PTBXLDataset(
            data=datasets["test_total"]["x"],
            labels=datasets["test_total"]["y"],
            mode='test'
        )

    elif config.data == "svhn":

        datasets, _ = load_SVHN(
            root=config.root,
            data_name=config.data,
            n_label_per_class=config.n_label_per_class,
            mismatch_ratio=config.mismatch_ratio,
            random_state=config.seed,
            logger=logger,
        )

        labeled_set = SVHN(
            data_name=config.data, dataset=datasets["l_train"], transform=train_trans
        )
        unlabeled_set = SVHN_STRONG(
            data_name=config.data, dataset=datasets["u_train"], transform=train_trans
        )
        eval_set = SVHN(
            data_name=config.data, dataset=datasets["validation"], transform=test_trans
        )
        test_set = SVHN(
            data_name=config.data, dataset=datasets["test"], transform=test_trans
        )
        open_test_set = SVHN(
            data_name=config.data, dataset=datasets["test_total"], transform=test_trans
        )

    else:
        raise NotImplementedError

    if local_rank == 0:
        logger.info(f"Data: {config.data}")
        logger.info(f"Labeled Data Observations: {len(labeled_set):,}")
        logger.info(f"Unlabeled Data Observations: {len(unlabeled_set):,}")
        logger.info(f"Backbone: {config.backbone_type}")
        logger.info(f"Checkpoint directory: {config.checkpoint_dir}")

    # Model (Task)
    model = Classification(backbone=model)
    model.prepare(
        ckpt_dir=config.checkpoint_dir,
        optimizer=config.optimizer,
        learning_rate=config.learning_rate,
        iterations=config.iterations,
        batch_size=config.batch_size,
        num_workers=config.num_workers,
        local_rank=local_rank,
        mixed_precision=config.mixed_precision,
        gamma=config.gamma,
        milestones=config.milestones,
        weight_decay=config.weight_decay,
    )

    # Train & evaluate
    start = time.time()
    model.run(
        train_set=[labeled_set, unlabeled_set],
        eval_set=eval_set,
        test_set=test_set,
        open_test_set=open_test_set,
        save_every=config.save_every,
        p_cutoff=config.p_cutoff,
        q_cutoff=config.q_cutoff,
        warm_up_end=config.warm_up,
        n_bins=config.n_bins,
        enable_plot=config.enable_plot,
        dist_da_len=config.dist_da_len,
        lambda_open=config.lambda_open,
        logger=logger,
    )
    elapsed_sec = time.time() - start

    if logger is not None:
        elapsed_mins = elapsed_sec / 60
        logger.info(f"Total training time: {elapsed_mins:,.2f} minutes.")
        logger.handlers.clear()


if __name__ == "__main__":

    try:
        main()
    except KeyboardInterrupt:
        sys.exit(0)
